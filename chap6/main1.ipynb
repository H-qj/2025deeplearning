{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80717497-7878-4f57-89d4-bb9b330892c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import collections\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "\n",
    "import rnn as rnn_lstm\n",
    "\n",
    "start_token = 'G'\n",
    "end_token = 'E'\n",
    "batch_size = 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c932203a-29f6-480f-91dd-1f1482b439f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_poems1(file_name):\n",
    "    \"\"\"\n",
    "\n",
    "    :param file_name:\n",
    "    :return: poems_vector  have tow dimmention ,first is the poem, the second is the word_index\n",
    "    e.g. [[1,2,3,4,5,6,7,8,9,10],[9,6,3,8,5,2,7,4,1]]\n",
    "\n",
    "    \"\"\"\n",
    "    poems = []\n",
    "    with open(file_name, \"r\", encoding='utf-8', ) as f:\n",
    "        for line in f.readlines():\n",
    "            try:\n",
    "                title, content = line.strip().split(':')\n",
    "                # content = content.replace(' ', '').replace('，','').replace('。','')\n",
    "                content = content.replace(' ', '')\n",
    "                if '_' in content or '(' in content or '（' in content or '《' in content or '[' in content or \\\n",
    "                                start_token in content or end_token in content:\n",
    "                    continue\n",
    "                if len(content) < 5 or len(content) > 80:\n",
    "                    continue\n",
    "                content = start_token + content + end_token\n",
    "                poems.append(content)\n",
    "            except ValueError as e:\n",
    "                print(\"error\")\n",
    "                pass\n",
    "    # 按诗的字数排序\n",
    "    poems = sorted(poems, key=lambda line: len(line))\n",
    "    # print(poems)\n",
    "    # 统计每个字出现次数\n",
    "    all_words = []\n",
    "    for poem in poems:\n",
    "        all_words += [word for word in poem]\n",
    "    counter = collections.Counter(all_words)  # 统计词和词频。\n",
    "    count_pairs = sorted(counter.items(), key=lambda x: -x[1])  # 排序\n",
    "    words, _ = zip(*count_pairs)\n",
    "    words = words[:len(words)] + (' ',)\n",
    "    word_int_map = dict(zip(words, range(len(words))))\n",
    "    poems_vector = [list(map(word_int_map.get, poem)) for poem in poems]\n",
    "    return poems_vector, word_int_map, words\n",
    "\n",
    "def process_poems2(file_name):\n",
    "    \"\"\"\n",
    "    :param file_name:\n",
    "    :return: poems_vector  have tow dimmention ,first is the poem, the second is the word_index\n",
    "    e.g. [[1,2,3,4,5,6,7,8,9,10],[9,6,3,8,5,2,7,4,1]]\n",
    "\n",
    "    \"\"\"\n",
    "    poems = []\n",
    "    with open(file_name, \"r\", encoding='utf-8', ) as f:\n",
    "        # content = ''\n",
    "        for line in f.readlines():\n",
    "            try:\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    content = line.replace(' '' ', '').replace('，','').replace('。','')\n",
    "                    if '_' in content or '(' in content or '（' in content or '《' in content or '[' in content or \\\n",
    "                                    start_token in content or end_token in content:\n",
    "                        continue\n",
    "                    if len(content) < 5 or len(content) > 80:\n",
    "                        continue\n",
    "                    # print(content)\n",
    "                    content = start_token + content + end_token\n",
    "                    poems.append(content)\n",
    "                    # content = ''\n",
    "            except ValueError as e:\n",
    "                # print(\"error\")\n",
    "                pass\n",
    "    # 按诗的字数排序\n",
    "    poems = sorted(poems, key=lambda line: len(line))\n",
    "    # print(poems)\n",
    "    # 统计每个字出现次数\n",
    "    all_words = []\n",
    "    for poem in poems:\n",
    "        all_words += [word for word in poem]\n",
    "    counter = collections.Counter(all_words)  # 统计词和词频。\n",
    "    count_pairs = sorted(counter.items(), key=lambda x: -x[1])  # 排序\n",
    "    words, _ = zip(*count_pairs)\n",
    "    words = words[:len(words)] + (' ',)\n",
    "    word_int_map = dict(zip(words, range(len(words))))\n",
    "    poems_vector = [list(map(word_int_map.get, poem)) for poem in poems]\n",
    "    return poems_vector, word_int_map, words\n",
    "\n",
    "def generate_batch(batch_size, poems_vec, word_to_int):\n",
    "    n_chunk = len(poems_vec) // batch_size\n",
    "    x_batches = []\n",
    "    y_batches = []\n",
    "    for i in range(n_chunk):\n",
    "        start_index = i * batch_size\n",
    "        end_index = start_index + batch_size\n",
    "        x_data = poems_vec[start_index:end_index]\n",
    "        y_data = []\n",
    "        for row in x_data:\n",
    "            y  = row[1:]\n",
    "            y.append(row[-1])\n",
    "            y_data.append(y)\n",
    "        \"\"\"\n",
    "        x_data             y_data\n",
    "        [6,2,4,6,9]       [2,4,6,9,9]\n",
    "        [1,4,2,8,5]       [4,2,8,5,5]\n",
    "        \"\"\"\n",
    "        # print(x_data[0])\n",
    "        # print(y_data[0])\n",
    "        # exit(0)\n",
    "        x_batches.append(x_data)\n",
    "        y_batches.append(y_data)\n",
    "    return x_batches, y_batches\n",
    "\n",
    "\n",
    "def run_training():\n",
    "    # 处理数据集\n",
    "    #poems_vector, word_to_int, vocabularies = process_poems2('./tangshi.txt')\n",
    "    poems_vector, word_to_int, vocabularies = process_poems1('./poems.txt')\n",
    "    # 生成batch\n",
    "    print(\"finish  loadding data\")\n",
    "    BATCH_SIZE = 100\n",
    "\n",
    "    torch.manual_seed(5)\n",
    "    word_embedding = rnn_lstm.word_embedding( vocab_length= len(word_to_int) + 1 , embedding_dim= 100)\n",
    "    rnn_model = rnn_lstm.RNN_model(batch_sz = BATCH_SIZE,vocab_len = len(word_to_int) + 1 ,word_embedding = word_embedding ,embedding_dim= 100, lstm_hidden_dim=128)\n",
    "\n",
    "    # optimizer = optim.Adam(rnn_model.parameters(), lr= 0.001)\n",
    "    optimizer=optim.RMSprop(rnn_model.parameters(), lr=0.01)\n",
    "\n",
    "    loss_fun = torch.nn.NLLLoss()\n",
    "    #rnn_model.load_state_dict(torch.load('./poem_generator_rnn'))  # if you have already trained your model you can load it by this line.\n",
    "\n",
    "    for epoch in range(30):\n",
    "        batches_inputs, batches_outputs = generate_batch(BATCH_SIZE, poems_vector, word_to_int)\n",
    "        n_chunk = len(batches_inputs)\n",
    "        for batch in range(n_chunk):\n",
    "            batch_x = batches_inputs[batch]\n",
    "            batch_y = batches_outputs[batch] # (batch , time_step)\n",
    "\n",
    "            loss = 0\n",
    "            for index in range(BATCH_SIZE):\n",
    "                x = np.array(batch_x[index], dtype = np.int64)\n",
    "                y = np.array(batch_y[index], dtype = np.int64)\n",
    "                x = Variable(torch.from_numpy(np.expand_dims(x,axis=1)))\n",
    "                y = Variable(torch.from_numpy(y ))\n",
    "                pre = rnn_model(x)\n",
    "                loss += loss_fun(pre , y)\n",
    "                if index == 0:\n",
    "                    _, pre = torch.max(pre, dim=1)\n",
    "                    print('prediction', pre.data.tolist()) # the following  three line can print the output and the prediction\n",
    "                    print('b_y       ', y.data.tolist())   # And you need to take a screenshot and then past is to your homework paper.\n",
    "                    print('*' * 30)\n",
    "            loss  = loss  / BATCH_SIZE\n",
    "            print(\"epoch  \",epoch,'batch number',batch,\"loss is: \", loss.data.tolist())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm(rnn_model.parameters(), 1)\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch % 20 ==0:\n",
    "                torch.save(rnn_model.state_dict(), './poem_generator_rnn')\n",
    "                print(\"finish  save model\")\n",
    "\n",
    "\n",
    "\n",
    "def to_word(predict, vocabs):  # 预测的结果转化成汉字\n",
    "    sample = np.argmax(predict)\n",
    "\n",
    "    if sample >= len(vocabs):\n",
    "        sample = len(vocabs) - 1\n",
    "\n",
    "    return vocabs[sample]\n",
    "\n",
    "\n",
    "def pretty_print_poem(poem):  # 令打印的结果更工整\n",
    "    shige=[]\n",
    "    for w in poem:\n",
    "        if w == start_token or w == end_token:\n",
    "            break\n",
    "        shige.append(w)\n",
    "    poem_sentences = poem.split('。')\n",
    "    for s in poem_sentences:\n",
    "        if s != '' and len(s) > 10:\n",
    "            print(s + '。')\n",
    "\n",
    "\n",
    "def gen_poem(begin_word):\n",
    "    #poems_vector, word_int_map, vocabularies = process_poems2('./tangshi.txt')  #  use the other dataset to train the network\n",
    "    poems_vector, word_int_map, vocabularies = process_poems1('./poems.txt')\n",
    "    word_embedding = rnn_lstm.word_embedding(vocab_length=len(word_int_map) + 1, embedding_dim=100)\n",
    "    rnn_model = rnn_lstm.RNN_model(batch_sz=64, vocab_len=len(word_int_map) + 1, word_embedding=word_embedding,\n",
    "                                   embedding_dim=100, lstm_hidden_dim=128)\n",
    "    \n",
    "    rnn_model.load_state_dict(torch.load('./poem_generator_rnn'))\n",
    "\n",
    "    # 指定开始的字\n",
    "\n",
    "    poem = begin_word\n",
    "    word = begin_word\n",
    "    while word != end_token:\n",
    "        input = np.array([word_int_map[w] for w in poem],dtype= np.int64)\n",
    "        input = Variable(torch.from_numpy(input))\n",
    "        output = rnn_model(input, is_test=True)\n",
    "        word = to_word(output.data.tolist()[-1], vocabularies)\n",
    "        poem += word\n",
    "        # print(word)\n",
    "        # print(poem)\n",
    "        if len(poem) > 30:\n",
    "            break\n",
    "    return poem\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90c457ca-35dd-46f9-bec0-8b3d326281cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run_training()  # 如果不是训练阶段 ，请注销这一行 。 网络训练时间很长。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ba506a8-a34c-4740-8f86-3acb66cdff33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error\n",
      "inital  linear weight \n",
      "日来长如何，一半一为一时。\n",
      "error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/environment/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inital  linear weight \n",
      "红桃花发露春，青山一望水山头。\n",
      "如今不见人间事，不是人间不得无。\n",
      "error\n",
      "inital  linear weight \n",
      "山上无人死，不是人间不得名。\n",
      "不是此中无一事，不能惆得是闲人。\n",
      "error\n",
      "inital  linear weight \n",
      "夜来看月中，一树风落一时开。\n",
      "error\n",
      "inital  linear weight \n",
      "湖海为人醉，一半千山一醉衣。\n",
      "不是此中无一事，不能惆得是闲人。\n",
      "error\n",
      "inital  linear weight \n",
      "海去无人，一半春风入洞庭。\n",
      "error\n",
      "inital  linear weight \n",
      "月已无名，不能闲去更何如。\n",
      "如今一别无人见，不是人间不得无。\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pretty_print_poem(gen_poem(\"日\"))\n",
    "pretty_print_poem(gen_poem(\"红\"))\n",
    "pretty_print_poem(gen_poem(\"山\"))\n",
    "pretty_print_poem(gen_poem(\"夜\"))\n",
    "pretty_print_poem(gen_poem(\"湖\"))\n",
    "pretty_print_poem(gen_poem(\"海\"))\n",
    "pretty_print_poem(gen_poem(\"月\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff2a9f8-350a-41c2-825c-80fcc617f16d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
